{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "from ocr_model import *\n",
    "from image_utils import *\n",
    "from random import randint\n",
    "from training_set_utils import *\n",
    "from PIL import Image, ImageEnhance, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OcrModel()\n",
    "model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "data_set = create_from_labeled_dir(\"images/labeled/\")\n",
    "data_set = letterbox_resize(data_set, 30, 42)\n",
    "data_set = data_set + create_inverted_images(data_set)\n",
    "data_set = equalize_classes(data_set)\n",
    "data_set = create_brightness_variations(data_set, 0.6, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#atlas_data_set = create_from_atlas_dir(\"images/atlas/\", 24, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#atlas_data_set = create_variations(atlas_data_set)\n",
    "#data_set = data_set + atlas_data_set\n",
    "#atlas_data_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "random.shuffle(data_set)\n",
    "\n",
    "num_test_data = 1000\n",
    "test_set = data_set[:num_test_data]\n",
    "test_xy = model.convert_data_set(test_set)\n",
    "\n",
    "train_set = data_set[num_test_data:]\n",
    "train_xy = model.convert_data_set(train_set)\n",
    "\n",
    "train_set = None\n",
    "data_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import math\n",
    "\n",
    "use_generator = True\n",
    "batch_size = 4 * 1024\n",
    "\n",
    "gen = ImageDataGenerator(\n",
    "        width_shift_range=3, \n",
    "        height_shift_range=3, \n",
    "        zoom_range=0.1, \n",
    "        horizontal_flip=False)\n",
    "        #rotation_range=10,\n",
    "        #brightness_range=[0.5, 1.0])\n",
    "\n",
    "last = time.time()\n",
    "for i in range(99999):\n",
    "\n",
    "    #use_generator = i % 2 == 1\n",
    "\n",
    "    if use_generator:        \n",
    "        model.cnn.fit_generator(gen.flow(\n",
    "            train_xy[0], train_xy[1],\n",
    "            batch_size=batch_size, shuffle=True), epochs=50, workers=24, steps_per_epoch=math.ceil(len(train_xy[0]) / batch_size))\n",
    "    else:\n",
    "        model.cnn.fit(\n",
    "            train_xy[0], train_xy[1],\n",
    "            epochs=50,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    now = time.time()\n",
    "    if now - last > 300:\n",
    "        last = now\n",
    "        loss_acc = model.cnn.evaluate(test_xy[0], test_xy[1],batch_size=batch_size)    \n",
    "        model.save_weights(\"acc\" + str(loss_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax2(arr):\n",
    "    idx0 = 0\n",
    "    last_max = 0.0\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] > last_max:\n",
    "            last_max = arr[i]\n",
    "            idx0 = i    \n",
    "    idx1 = 0\n",
    "    last_max = 0.0\n",
    "    for i in range(len(arr)):\n",
    "        if i != idx0 and arr[i] > last_max:\n",
    "            last_max = arr[i]\n",
    "            idx1 = i\n",
    "    return (idx0, idx1)\n",
    "\n",
    "def confidence(arr, pred):\n",
    "    conv0 = arr[pred[0]]\n",
    "    conv1 = arr[pred[1]]\n",
    "    return (round(conv0, 2), round(conv1, 2))\n",
    "\n",
    "def predict(img):\n",
    "    result = model.predict(img)\n",
    "    pred = argmax2(result)\n",
    "    return (pred, confidence(result, pred))\n",
    "\n",
    "def predict_test(idx):\n",
    "    result = predict(test_set[idx].image)\n",
    "    pred = result[0]\n",
    "    conf = result[1]\n",
    "    expect = argmax2(test_set[idx].y)\n",
    "    expect_conf = confidence(test_set[idx].y, expect)\n",
    "    if expect_conf[0] > 0.6:\n",
    "        correct = expect[0] == pred[0]\n",
    "        print(\"correct: \" + str(correct) + \n",
    "              \"; expected: \" + str(expect) + \" \" + str(expect_conf) +\n",
    "              \"; predicted: \" + str(pred) + \" \" + str(conf))\n",
    "        if not correct: # or convidence < 0.9:\n",
    "            test_set[idx].show()\n",
    "            #plt.imshow(x_test[idx])\n",
    "            #plt.show()\n",
    "    else:\n",
    "        print(\"skip\")\n",
    "for i in range(len(test_set)):\n",
    "    predict_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights()\n",
    "\n",
    "model.export_as_tflite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python373jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}